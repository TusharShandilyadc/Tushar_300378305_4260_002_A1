{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64908a8c",
   "metadata": {},
   "source": [
    "# **Stock Price Prediction: Research, Benchmarking, and Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ae05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "import polars as pl\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a20e9ef",
   "metadata": {},
   "source": [
    "## **Part 1: Storing and Retrieving Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"all_stocks_5yr.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_size = os.path.getsize(csv_path) / (1024 * 1024)\n",
    "print(f\"CSV File Size (1x): {csv_size:.2f} MB\")\n",
    "\n",
    "# Convert to Parquet with Gzip Compression\n",
    "df.to_parquet(\"all_stocks_5yr.parquet\", engine='pyarrow', compression='gzip', index=False)\n",
    "parquet_size = os.path.getsize(\"all_stocks_5yr.parquet\") / (1024 * 1024)\n",
    "print(f\"Parquet File Size (1x, Gzip): {parquet_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877ea6f",
   "metadata": {},
   "source": [
    "## **Benchmarking CSV vs Parquet (1x, 10x, 100x)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10x = pd.concat([df] * 10, ignore_index=True)\n",
    "df_100x = pd.concat([df] * 100, ignore_index=True)\n",
    "\n",
    "df_10x.to_csv(\"all_stocks_5yr_10x.csv\", index=False)\n",
    "df_10x.to_parquet(\"all_stocks_5yr_10x.parquet\", compression='gzip')\n",
    "\n",
    "df_100x.to_csv(\"all_stocks_5yr_100x.csv\", index=False)\n",
    "df_100x.to_parquet(\"all_stocks_5yr_100x.parquet\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd75fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results = []\n",
    "\n",
    "def benchmark_read_write(file_path, read_func, write_func, label):\n",
    "    start_time = time.time()\n",
    "    df = read_func(file_path)\n",
    "    read_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    write_func(df, file_path)\n",
    "    write_time = time.time() - start_time\n",
    "    \n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    benchmark_results.append([label, file_size, read_time, write_time])\n",
    "\n",
    "benchmark_read_write(\"all_stocks_5yr.csv\", pd.read_csv, lambda df, path: df.to_csv(path, index=False), \"CSV 1x\")\n",
    "benchmark_read_write(\"all_stocks_5yr.parquet\", pd.read_parquet, lambda df, path: df.to_parquet(path, compression='gzip'), \"Parquet 1x\")\n",
    "benchmark_read_write(\"all_stocks_5yr_10x.csv\", pd.read_csv, lambda df, path: df.to_csv(path, index=False), \"CSV 10x\")\n",
    "benchmark_read_write(\"all_stocks_5yr_10x.parquet\", pd.read_parquet, lambda df, path: df.to_parquet(path, compression='gzip'), \"Parquet 10x\")\n",
    "benchmark_read_write(\"all_stocks_5yr_100x.csv\", pd.read_csv, lambda df, path: df.to_csv(path, index=False), \"CSV 100x\")\n",
    "benchmark_read_write(\"all_stocks_5yr_100x.parquet\", pd.read_parquet, lambda df, path: df.to_parquet(path, compression='gzip'), \"Parquet 100x\")\n",
    "\n",
    "benchmark_df = pd.DataFrame(benchmark_results, columns=[\"Dataset\", \"File Size (MB)\", \"Read Time (s)\", \"Write Time (s)\"])\n",
    "benchmark_df.to_csv(\"benchmark_results.csv\", index=False)\n",
    "benchmark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458224a0",
   "metadata": {},
   "source": [
    "## **Part 2: Data Manipulation & Pandas vs Polars Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_pandas = pd.read_parquet(\"all_stocks_5yr.parquet\")\n",
    "pandas_load_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "df_polars = pl.read_parquet(\"all_stocks_5yr.parquet\")\n",
    "polars_load_time = time.time() - start_time\n",
    "\n",
    "benchmark_pandas_polars = pd.DataFrame({\n",
    "    \"Library\": [\"Pandas\", \"Polars\"],\n",
    "    \"Load Time (s)\": [pandas_load_time, polars_load_time]\n",
    "})\n",
    "benchmark_pandas_polars.to_csv(\"pandas_vs_polars.csv\", index=False)\n",
    "benchmark_pandas_polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa21fbb",
   "metadata": {},
   "source": [
    "## **Enhancing Data with Technical Indicators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcee3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SMA_20'] = df['close'].rolling(20).mean()\n",
    "df['EMA_20'] = df['close'].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df['RSI_14'] = calculate_rsi(df['close'])\n",
    "df['MACD'] = df['close'].ewm(span=12, adjust=False).mean() - df['close'].ewm(span=26, adjust=False).mean()\n",
    "df = df.dropna()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8ff2e",
   "metadata": {},
   "source": [
    "## **Part 3: Building & Evaluating Prediction Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c776ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"open\", \"high\", \"low\", \"volume\", \"SMA_20\", \"EMA_20\", \"RSI_14\", \"MACD\"]\n",
    "target = \"close\"\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# Train Gradient Boosting Model\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "gbr_model.fit(X_train_scaled, y_train)\n",
    "joblib.dump(gbr_model, \"gbr_model.pkl\")\n",
    "\n",
    "# Train XGBoost Model\n",
    "xgb_model = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "joblib.dump(xgb_model, \"xgb_model.pkl\")\n",
    "\n",
    "print(\"âœ… Gradient Boosting and XGBoost Models Saved Correctly!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}